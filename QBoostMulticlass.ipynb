{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Home/Documents/QML/Hackathon/qml_hackathon\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X_tr = np.load('X_train.npy')\n",
    "X_te = np.load('X_test.npy')\n",
    "y1_tr = np.load('y1_train.npy')\n",
    "y2_tr = np.load('y2_train.npy')\n",
    "\n",
    "# Fill in your API token:\n",
    "\n",
    "\n",
    "sapi_token = 'CDL8-df1de1d5d76560ee73a82ffca3833a1a444536d3'\n",
    "url = 'https://cloud.dwavesys.com/sapi'\n",
    "token = sapi_token\n",
    "solver_name = 'c4-sw_sample'#'DW_2000Q_2'\n",
    "\n",
    "# import necessary packages\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.datasets.mldata import fetch_mldata\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from dwave.system.samplers import DWaveSampler\n",
    "from dwave.system.composites import EmbeddingComposite\n",
    "\n",
    "from qboost import WeakClassifiers, QBoostClassifier, QboostPlus\n",
    "\n",
    "\n",
    "# Define the functions required in this example\n",
    "\n",
    "def metric(y, y_pred):\n",
    "    \"\"\"\n",
    "    :param y: true label\n",
    "    :param y_pred: predicted label\n",
    "    :return: metric score\n",
    "    \"\"\"\n",
    "\n",
    "    return metrics.accuracy_score(y, y_pred)\n",
    "\n",
    "# performance metric\n",
    "\n",
    "def train_model(X_train, y_train, X_test, y_test, lmd):\n",
    "    \"\"\"\n",
    "    :param X_train: training data\n",
    "    :param y_train: training label\n",
    "    :param X_test: testing data\n",
    "    :param y_test: testing label\n",
    "    :param lmd: lambda used in regularization\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # define parameters used in this function\n",
    "    NUM_READS = 1000\n",
    "    NUM_WEAK_CLASSIFIERS = 30\n",
    "    TREE_DEPTH = 4\n",
    "    DW_PARAMS = {'num_reads': NUM_READS,\n",
    "                 'auto_scale': True,\n",
    "                 'num_spin_reversal_transforms': 10,\n",
    "                 'postprocess': 'optimization',\n",
    "                 }\n",
    "\n",
    "    # define sampler\n",
    "    dwave_sampler = DWaveSampler(token=sapi_token, endpoint = url)\n",
    "    emb_sampler = EmbeddingComposite(dwave_sampler)\n",
    "\n",
    "    N_train = len(X_train)\n",
    "    N_test = len(X_test)\n",
    "    print(\"\\n======================================\")\n",
    "    print(\"Train size: %d, Test size: %d\" %(N_train, N_test))\n",
    "    print('Num weak classifiers:', NUM_WEAK_CLASSIFIERS)\n",
    "\n",
    "    # Preprocessing data\n",
    "    imputer = preprocessing.Imputer()\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    normalizer = preprocessing.Normalizer()\n",
    "\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_train = normalizer.fit_transform(X_train)\n",
    "\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    X_test = normalizer.fit_transform(X_test)\n",
    "    \n",
    "    ## Adaboost\n",
    "    print('\\nAdaboost')\n",
    "    clf1 = AdaBoostClassifier(n_estimators=NUM_WEAK_CLASSIFIERS)\n",
    "    clf1.fit(X_train, y_train)\n",
    "    y_train1 = clf1.predict(X_train)\n",
    "    y_test1 = clf1.predict(X_test)\n",
    "#     print(clf1.estimator_weights_)\n",
    "    print('accu (train): %5.2f'%(metric(y_train, y_train1)))\n",
    "    print('accu (test): %5.2f'%(metric(y_test, y_test1)))\n",
    "\n",
    "        # Ensembles of Decision Tree\n",
    "    print('\\nDecision tree')\n",
    "    clf2 = WeakClassifiers(n_estimators=NUM_WEAK_CLASSIFIERS, max_depth=TREE_DEPTH)\n",
    "    clf2.fit(X_train, y_train)\n",
    "    y_train2 = clf2.predict(X_train)\n",
    "    y_test2 = clf2.predict(X_test)\n",
    "#     print(clf2.estimator_weights)\n",
    "    print('accu (train): %5.2f' % (metric(y_train, y_train2)))\n",
    "    print('accu (test): %5.2f' % (metric(y_test, y_test2)))\n",
    "    \n",
    "    # Random forest\n",
    "    print('\\nRandom Forest')\n",
    "    clf3 = RandomForestClassifier(max_depth=TREE_DEPTH, n_estimators=NUM_WEAK_CLASSIFIERS)\n",
    "    clf3.fit(X_train, y_train)\n",
    "    y_train3 = clf3.predict(X_train)\n",
    "    y_test3 = clf3.predict(X_test)\n",
    "    print('accu (train): %5.2f' % (metric(y_train, y_train3)))\n",
    "    print('accu (test): %5.2f' % (metric(y_test, y_test3)))\n",
    "    \n",
    "    # Qboost\n",
    "    print('\\nQBoost')\n",
    "    clf4 = QBoostClassifier(n_estimators=NUM_WEAK_CLASSIFIERS, max_depth=TREE_DEPTH)\n",
    "    clf4.fit(X_train, y_train, emb_sampler, lmd=lmd, **DW_PARAMS)\n",
    "    y_train4 = clf4.predict(X_train)\n",
    "    y_test4 = clf4.predict(X_test)\n",
    "    print(clf4.estimator_weights)\n",
    "    print('accu (train): %5.2f' % (metric(y_train, y_train4)))\n",
    "    print('accu (test): %5.2f' % (metric(y_test, y_test4)))\n",
    "\n",
    "#    QboostPlus\n",
    "    print('\\nQBoostPlus')\n",
    "    clf5 = QboostPlus([clf1, clf2, clf3, clf4])\n",
    "    clf5.fit(X_train, y_train, emb_sampler, lmd=lmd, **DW_PARAMS)\n",
    "    y_train5 = clf5.predict(X_train)\n",
    "    y_test5 = clf5.predict(X_test)\n",
    "    print(clf5.estimator_weights)\n",
    "    print('accu (train): %5.2f' % (metric(y_train, y_train5)))\n",
    "    print('accu (test): %5.2f' % (metric(y_test, y_test5)))\n",
    "\n",
    "    \n",
    "    return [clf4]\n",
    "\n",
    "# start training the model\n",
    "\n",
    "idx = np.arange(len(X_tr))\n",
    "np.random.shuffle(idx)  # shuffles index\n",
    "\n",
    "y_train = y1_tr\n",
    "y_bin = 2*(y1_tr >0.25) - 1\n",
    "\n",
    "X_train = X_tr[:int(len(idx)*.8)]\n",
    "y_train = y_bin[:int(len(idx)*.8)]\n",
    "\n",
    "X_test = X_tr[int(len(idx)*.8):]\n",
    "y_test = y_bin[int(len(idx)*.8):]\n",
    "\n",
    "# start training the model\n",
    "#X_train = X_tr\n",
    "#y_train = y1_tr\n",
    "#y_train = 2*(y_train >0.25) - 1\n",
    "#X_test = X_train\n",
    "#y_test = y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================\n",
      "Train size: 1920, Test size: 480\n",
      "('Num weak classifiers:', 30)\n",
      "\n",
      "Adaboost\n",
      "accu (train):  0.93\n",
      "accu (test):  0.89\n",
      "\n",
      "Decision tree\n",
      "accu (train):  0.97\n",
      "accu (test):  0.91\n",
      "\n",
      "Random Forest\n",
      "accu (train):  0.92\n",
      "accu (test):  0.90\n",
      "\n",
      "QBoost\n",
      "[1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1]\n",
      "accu (train):  0.94\n",
      "accu (test):  0.88\n",
      "\n",
      "QBoostPlus\n",
      "[1 1 1 1]\n",
      "accu (train):  0.95\n",
      "accu (test):  0.91\n"
     ]
    }
   ],
   "source": [
    "clfs = train_model(X_train, y_train, X_test, y_test, 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 3, ..., 2, 5, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create bins and allocate y to each bin\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "\n",
    "y = y1_tr\n",
    "x = X_tr\n",
    "\n",
    "# split into n_classes, or into n_classes bins\n",
    "n_classes = 10\n",
    "bins = np.linspace(np.min(y), np.max(y), num=n_classes)\n",
    "inds = np.digitize(y, bins)\n",
    "\n",
    "inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 10\n",
      "480\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# implement one versus rest classifier\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "y = inds # label_binarize(inds, classes=bins)\n",
    "#n_classes = y.shape[1]\n",
    "print np.min(y),np.max(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.2,\n",
    "                                                    random_state=0)\n",
    "print len(X_test)\n",
    "\n",
    "#OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True))\n",
    "#y_score = classifier.fit(X_train, y_train)\n",
    "#y_pred = classifier.predict(X_test)\n",
    "#acc = accuracy_score(y_test, y_pred)\n",
    "#print acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================\n",
      "Train size: 1920, Test size: 480\n",
      "('Num weak classifiers:', 30)\n",
      "\n",
      "Adaboost\n",
      "accu (train):  0.29\n",
      "accu (test):  0.27\n",
      "\n",
      "Decision tree\n",
      "accu (train):  0.10\n",
      "accu (test):  0.08\n",
      "\n",
      "Random Forest\n",
      "accu (train):  0.61\n",
      "accu (test):  0.51\n",
      "\n",
      "QBoost\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "accu (train):  0.10\n",
      "accu (test):  0.08\n",
      "\n",
      "QBoostPlus\n",
      "[1 1 1 1]\n",
      "accu (train):  0.10\n",
      "accu (test):  0.08\n"
     ]
    }
   ],
   "source": [
    "# run qboost with dicrete variables\n",
    "# does not work well\n",
    "\n",
    "clfs = train_model(X_train, y_train, X_test, y_test, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 2 1 6 3 1 2 5 6 4 4 4 2 2 3 2 7 3 4 2 3 2 4 1 1 3 4 4 1 4 3 4 4 4 3 2\n",
      " 6 2 5 1 2 2 4 1 5 1 3 2 4 1 6 2 4 4 2 1 2 3 2 2 2 2 3 6 4 3 4 4 3 6 3 6 2\n",
      " 3 5 5 2 4 2 1 4 2 3 3 3 5 3 4 4 2 3 5 5 4 3 4 3 3 3 4 2 3 4 6 5 4 3 5 6 1\n",
      " 4 3 3 2 1 4 5 5 2 4 2 4 5 6 5 4 2 3 6 3 5 2 4 5 2 7 4 4 2 6 3 1 6 2 2 2 2\n",
      " 5 4 5 3 5 5 5 4 3 3 6 2 3 4 4 6 2 2 4 6 2 3 2 4 5 7 4 4 3 4 2 2 6 5 2 4 4\n",
      " 5 5 4 3 5 4 4 3 3 3 2 2 2 1 3 1 2 3 3 3 6 3 4 5 4 3 4 2 3 5 5 4 3 3 4 3 2\n",
      " 4 1 2 1 2 4 3 3 4 4 5 4 4 4 3 3 2 4 2 2 6 3 3 5 1 2 3 1 4 5 4 4 2 3 4 7 4\n",
      " 2 6 4 2 3 6 2 6 4 2 4 2 5 3 4 4 2 7 2 2 4 4 4 6 3 4 4 4 4 5 3 6 2 5 2 3 5\n",
      " 4 2 3 1 2 3 2 4 5 6 3 4 3 5 3 1 4 2 3 2 4 4 6 1 3 4 2 2 1 5 6 2 3 4 2 3 5\n",
      " 3 6 3 5 3 4 2 2 2 2 5 5 3 3 2 4 4 2 3 3 4 6 4 3 4 3 2 2 4 4 4 3 2 2 3 4 4\n",
      " 2 2 6 4 2 4 4 2 6 5 5 2 2 2 4 2 5 1 3 2 5 4 2 6 3 3 4 5 5 3 2 3 6 4 3 5 4\n",
      " 3 4 2 2 4 3 3 3 4 5 3 6 3 4 1 4 3 4 3 6 1 4 2 6 2 4 3 3 3 5 5 2 4 1 3 5 6\n",
      " 4 4 4 6 2 4 5 1 4 4 4 3 5 4 3 1 6 7 1 3 4 6 3 4 3 3 5 3 3 4 4 6 4 6 6 5] 0.6125\n"
     ]
    }
   ],
   "source": [
    "# one vs rest classifer with AdaBoost\n",
    "# works well, but low performance due to unbalances classes \n",
    "NUM_WEAK_CLASSIFIERS=30\n",
    "classifier = OneVsRestClassifier(AdaBoostClassifier(n_estimators=NUM_WEAK_CLASSIFIERS))\n",
    "y_score = classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print y_pred, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Qboost:  1 / 3\n",
      "Working on Qboost:  2 / 3\n",
      "Working on Qboost:  3 / 3\n",
      "[1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 2, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2]\n",
      "Accuracy Score: 0.89375\n"
     ]
    }
   ],
   "source": [
    "## One vs Rest classifier\n",
    "\n",
    "y = y1_tr\n",
    "x = X_tr\n",
    "\n",
    "# split into n_classes, or into n_classes bins\n",
    "n_classes = 3\n",
    "bins = np.linspace(np.min(y), np.max(y), num=n_classes)\n",
    "inds = np.digitize(y, bins)\n",
    "\n",
    "\n",
    "# split data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, inds, test_size=.2,\n",
    "                                                    random_state=0)\n",
    "##\n",
    "\n",
    "NUM_READS = 1000\n",
    "NUM_WEAK_CLASSIFIERS = 30\n",
    "TREE_DEPTH = 4\n",
    "DW_PARAMS = {'num_reads': NUM_READS,\n",
    "             'auto_scale': True,\n",
    "             'num_spin_reversal_transforms': 10,\n",
    "             'postprocess': 'optimization',\n",
    "             }\n",
    "\n",
    "# define sampler\n",
    "dwave_sampler = DWaveSampler(token=sapi_token, endpoint = url)\n",
    "emb_sampler = EmbeddingComposite(dwave_sampler)\n",
    "lmd = 0.2\n",
    "\n",
    "classifiers = []\n",
    "predictions = []\n",
    "pred_test_labels = [0]*len(y_test)\n",
    "pred_train_labels = [0]*len(y_train)\n",
    "\n",
    "for i in range(n_classes):\n",
    "    print 'Working on Qboost: ',i+1,'/',n_classes\n",
    "    new_label = 2*(y_train==i+1)-1\n",
    "     \n",
    "    new_label = np.array(new_label)\n",
    "    clf = QBoostClassifier(n_estimators=NUM_WEAK_CLASSIFIERS, max_depth=TREE_DEPTH)\n",
    "    clf.fit(X_train, new_label, emb_sampler, lmd=lmd, **DW_PARAMS)\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    classifiers.append(clf)\n",
    "    predictions.append(y_test_pred)\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    for j in range(len(predictions[i])):\n",
    "        if predictions[i][j] == 1:\n",
    "            pred_test_labels[j] = i+1\n",
    "            \n",
    "print pred_test_labels\n",
    "acc = accuracy_score(y_test, pred_test_labels)\n",
    "\n",
    "print 'Accuracy Score:', acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, ..., -1, -1, -1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def qboost_single(X_train, y_train, X_test, y_test, lmd):\n",
    "    \"\"\"\n",
    "    :param X_train: training data\n",
    "    :param y_train: training label\n",
    "    :param X_test: testing data\n",
    "    :param y_test: testing label\n",
    "    :param lmd: lambda used in regularization\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # define parameters used in this function\n",
    "    NUM_READS = 1000\n",
    "    NUM_WEAK_CLASSIFIERS = 30\n",
    "    TREE_DEPTH = 4\n",
    "    DW_PARAMS = {'num_reads': NUM_READS,\n",
    "                 'auto_scale': True,\n",
    "                 'num_spin_reversal_transforms': 10,\n",
    "                 'postprocess': 'optimization',\n",
    "                 }\n",
    "\n",
    "    # define sampler\n",
    "    dwave_sampler = DWaveSampler(token=sapi_token, endpoint = url)\n",
    "    emb_sampler = EmbeddingComposite(dwave_sampler)\n",
    "\n",
    "    N_train = len(X_train)\n",
    "    N_test = len(X_test)\n",
    "    print(\"\\n======================================\")\n",
    "    print(\"Train size: %d, Test size: %d\" %(N_train, N_test))\n",
    "    print('Num weak classifiers:', NUM_WEAK_CLASSIFIERS)\n",
    "\n",
    "    # Preprocessing data\n",
    "    imputer = preprocessing.Imputer()\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    normalizer = preprocessing.Normalizer()\n",
    "\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_train = normalizer.fit_transform(X_train)\n",
    "\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    X_test = normalizer.fit_transform(X_test)\n",
    "    \n",
    "    ## Adaboost\n",
    "    print('\\nAdaboost')\n",
    "    clf1 = AdaBoostClassifier(n_estimators=NUM_WEAK_CLASSIFIERS)\n",
    "    clf1.fit(X_train, y_train)\n",
    "    y_train1 = clf1.predict(X_train)\n",
    "    y_test1 = clf1.predict(X_test)\n",
    "#     print(clf1.estimator_weights_)\n",
    "    print('accu (train): %5.2f'%(metric(y_train, y_train1)))\n",
    "    print('accu (test): %5.2f'%(metric(y_test, y_test1)))\n",
    "\n",
    "        # Ensembles of Decision Tree\n",
    "    print('\\nDecision tree')\n",
    "    clf2 = WeakClassifiers(n_estimators=NUM_WEAK_CLASSIFIERS, max_depth=TREE_DEPTH)\n",
    "    clf2.fit(X_train, y_train)\n",
    "    y_train2 = clf2.predict(X_train)\n",
    "    y_test2 = clf2.predict(X_test)\n",
    "#     print(clf2.estimator_weights)\n",
    "    print('accu (train): %5.2f' % (metric(y_train, y_train2)))\n",
    "    print('accu (test): %5.2f' % (metric(y_test, y_test2)))\n",
    "    \n",
    "    # Random forest\n",
    "    print('\\nRandom Forest')\n",
    "    clf3 = RandomForestClassifier(max_depth=TREE_DEPTH, n_estimators=NUM_WEAK_CLASSIFIERS)\n",
    "    clf3.fit(X_train, y_train)\n",
    "    y_train3 = clf3.predict(X_train)\n",
    "    y_test3 = clf3.predict(X_test)\n",
    "    print('accu (train): %5.2f' % (metric(y_train, y_train3)))\n",
    "    print('accu (test): %5.2f' % (metric(y_test, y_test3)))\n",
    "    \n",
    "    # Qboost\n",
    "    print('\\nQBoost')\n",
    "    clf4 = QBoostClassifier(n_estimators=NUM_WEAK_CLASSIFIERS, max_depth=TREE_DEPTH)\n",
    "    clf4.fit(X_train, y_train, emb_sampler, lmd=lmd, **DW_PARAMS)\n",
    "    y_train4 = clf4.predict(X_train)\n",
    "    y_test4 = clf4.predict(X_test)\n",
    "    print(clf4.estimator_weights)\n",
    "    print('accu (train): %5.2f' % (metric(y_train, y_train4)))\n",
    "    print('accu (test): %5.2f' % (metric(y_test, y_test4)))\n",
    "\n",
    "#    QboostPlus\n",
    "    print('\\nQBoostPlus')\n",
    "    clf5 = QboostPlus([clf1, clf2, clf3, clf4])\n",
    "    clf5.fit(X_train, y_train, emb_sampler, lmd=lmd, **DW_PARAMS)\n",
    "    y_train5 = clf5.predict(X_train)\n",
    "    y_test5 = clf5.predict(X_test)\n",
    "    print(clf5.estimator_weights)\n",
    "    print('accu (train): %5.2f' % (metric(y_train, y_train5)))\n",
    "    print('accu (test): %5.2f' % (metric(y_test, y_test5)))\n",
    "    \n",
    "#    Q_OneVsRest Plus\n",
    "    print('\\nQ_OneVsRest')\n",
    "    clf6 = OneVsRestClassifier(QBoostClassifier(n_estimators=NUM_WEAK_CLASSIFIERS, max_depth=TREE_DEPTH))\n",
    "    clf6.fit(X_train, y_train, emb_sampler, lmd=lmd, **DW_PARAMS)\n",
    "    y_train6 = clf6.predict(X_train)\n",
    "    y_test6 = clf6.predict(X_test)\n",
    "    print(clf6.estimator_weights)\n",
    "\n",
    "    \n",
    "    return [clf4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================\n",
      "Train size: 1920, Test size: 480\n",
      "('Num weak classifiers:', 30)\n",
      "\n",
      "Adaboost\n",
      "accu (train):  0.29\n",
      "accu (test):  0.27\n",
      "\n",
      "Decision tree\n",
      "accu (train):  0.10\n",
      "accu (test):  0.08\n",
      "\n",
      "Random Forest\n",
      "accu (train):  0.62\n",
      "accu (test):  0.54\n",
      "\n",
      "QBoost\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "accu (train):  0.10\n",
      "accu (test):  0.08\n",
      "\n",
      "QBoostPlus\n",
      "[1 1 1 1]\n",
      "accu (train):  0.10\n",
      "accu (test):  0.08\n"
     ]
    }
   ],
   "source": [
    "clfs = train_model(X_train, y_train, X_test, y_test, 1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named BaseEstimator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-8a004415b33a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClassifierMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMetaEstimatorMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_regressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named BaseEstimator"
     ]
    }
   ],
   "source": [
    "import array\n",
    "import numpy as np\n",
    "import warnings\n",
    "import scipy.sparse as sp\n",
    "import itertools\n",
    "\n",
    "import BaseEstimator, ClassifierMixin, clone, is_classifier\n",
    "from .base import MetaEstimatorMixin, is_regressor\n",
    "from .preprocessing import LabelBinarizer\n",
    "from .metrics.pairwise import euclidean_distances\n",
    "from .utils import check_random_state\n",
    "from .utils.validation import _num_samples\n",
    "from .utils.validation import check_is_fitted\n",
    "from .utils.validation import check_X_y, check_array\n",
    "from .utils.multiclass import (_check_partial_fit_first_call,\n",
    "                               check_classification_targets,\n",
    "                               _ovr_decision_function)\n",
    "from .utils.metaestimators import _safe_split, if_delegate_has_method\n",
    "\n",
    "from .externals.joblib import Parallel\n",
    "from .externals.joblib import delayed\n",
    "from .externals.six.moves import zip as izip\n",
    "\n",
    "class OneVsRestClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):\n",
    "    \"\"\"One-vs-the-rest (OvR) multiclass/multilabel strategy\n",
    "    Also known as one-vs-all, this strategy consists in fitting one classifier\n",
    "    per class. For each classifier, the class is fitted against all the other\n",
    "    classes. In addition to its computational efficiency (only `n_classes`\n",
    "    classifiers are needed), one advantage of this approach is its\n",
    "    interpretability. Since each class is represented by one and one classifier\n",
    "    only, it is possible to gain knowledge about the class by inspecting its\n",
    "    corresponding classifier. This is the most commonly used strategy for\n",
    "    multiclass classification and is a fair default choice.\n",
    "    This strategy can also be used for multilabel learning, where a classifier\n",
    "    is used to predict multiple labels for instance, by fitting on a 2-d matrix\n",
    "    in which cell [i, j] is 1 if sample i has label j and 0 otherwise.\n",
    "    In the multilabel learning literature, OvR is also known as the binary\n",
    "    relevance method.\n",
    "    Read more in the :ref:`User Guide <ovr_classification>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object\n",
    "        An estimator object implementing `fit` and one of `decision_function`\n",
    "        or `predict_proba`.\n",
    "    n_jobs : int, optional, default: 1\n",
    "        The number of jobs to use for the computation. If -1 all CPUs are used.\n",
    "        If 1 is given, no parallel computing code is used at all, which is\n",
    "        useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are\n",
    "        used. Thus for n_jobs = -2, all CPUs but one are used.\n",
    "    Attributes\n",
    "    ----------\n",
    "    estimators_ : list of `n_classes` estimators\n",
    "        Estimators used for predictions.\n",
    "    classes_ : array, shape = [`n_classes`]\n",
    "        Class labels.\n",
    "    label_binarizer_ : LabelBinarizer object\n",
    "        Object used to transform multiclass labels to binary labels and\n",
    "        vice-versa.\n",
    "    multilabel_ : boolean\n",
    "        Whether a OneVsRestClassifier is a multilabel classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, estimator, n_jobs=1):\n",
    "        self.estimator = estimator\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, X, y,emb_sampler, lmd=lmd, **DW_PARAMS):\n",
    "        \"\"\"Fit underlying estimators.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (sparse) array-like, shape = [n_samples, n_features]\n",
    "            Data.\n",
    "        y : (sparse) array-like, shape = [n_samples, ], [n_samples, n_classes]\n",
    "            Multi-class targets. An indicator matrix turns on multilabel\n",
    "            classification.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        # A sparse LabelBinarizer, with sparse_output=True, has been shown to\n",
    "        # outpreform or match a dense label binarizer in all cases and has also\n",
    "        # resulted in less or equal memory consumption in the fit_ovr function\n",
    "        # overall.\n",
    "        self.label_binarizer_ = LabelBinarizer(sparse_output=True)\n",
    "        Y = self.label_binarizer_.fit_transform(y)\n",
    "        Y = Y.tocsc()\n",
    "        self.classes_ = self.label_binarizer_.classes_\n",
    "        columns = (col.toarray().ravel() for col in Y.T)\n",
    "        # In cases where individual estimators are very fast to train setting\n",
    "        # n_jobs > 1 in can results in slower performance due to the overhead\n",
    "        # of spawning threads.  See joblib issue #112.\n",
    "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(delayed(_fit_binary)(\n",
    "            self.estimator, X, column, classes=[\n",
    "                \"not %s\" % self.label_binarizer_.classes_[i],\n",
    "                self.label_binarizer_.classes_[i]])\n",
    "            for i, column in enumerate(columns))\n",
    "\n",
    "        return self\n",
    "\n",
    "    @if_delegate_has_method('estimator')\n",
    "    def partial_fit(self, X, y, classes=None):\n",
    "        \"\"\"Partially fit underlying estimators\n",
    "        Should be used when memory is inefficient to train all data.\n",
    "        Chunks of data can be passed in several iteration.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (sparse) array-like, shape = [n_samples, n_features]\n",
    "            Data.\n",
    "        y : (sparse) array-like, shape = [n_samples, ], [n_samples, n_classes]\n",
    "            Multi-class targets. An indicator matrix turns on multilabel\n",
    "            classification.\n",
    "        classes : array, shape (n_classes, )\n",
    "            Classes across all calls to partial_fit.\n",
    "            Can be obtained via `np.unique(y_all)`, where y_all is the\n",
    "            target vector of the entire dataset.\n",
    "            This argument is only required in the first call of partial_fit\n",
    "            and can be omitted in the subsequent calls.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        if _check_partial_fit_first_call(self, classes):\n",
    "            if not hasattr(self.estimator, \"partial_fit\"):\n",
    "                raise ValueError((\"Base estimator {0}, doesn't have \"\n",
    "                                 \"partial_fit method\").format(self.estimator))\n",
    "            self.estimators_ = [clone(self.estimator) for _ in range\n",
    "                                (self.n_classes_)]\n",
    "\n",
    "            # A sparse LabelBinarizer, with sparse_output=True, has been\n",
    "            # shown to outperform or match a dense label binarizer in all\n",
    "            # cases and has also resulted in less or equal memory consumption\n",
    "            # in the fit_ovr function overall.\n",
    "            self.label_binarizer_ = LabelBinarizer(sparse_output=True)\n",
    "            self.label_binarizer_.fit(self.classes_)\n",
    "\n",
    "        if len(np.setdiff1d(y, self.classes_)):\n",
    "            raise ValueError((\"Mini-batch contains {0} while classes \" +\n",
    "                             \"must be subset of {1}\").format(np.unique(y),\n",
    "                                                             self.classes_))\n",
    "\n",
    "        Y = self.label_binarizer_.transform(y)\n",
    "        Y = Y.tocsc()\n",
    "        columns = (col.toarray().ravel() for col in Y.T)\n",
    "\n",
    "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(_partial_fit_binary)(estimator, X, column)\n",
    "            for estimator, column in izip(self.estimators_, columns))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict multi-class targets using underlying estimators.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (sparse) array-like, shape = [n_samples, n_features]\n",
    "            Data.\n",
    "        Returns\n",
    "        -------\n",
    "        y : (sparse) array-like, shape = [n_samples, ], [n_samples, n_classes].\n",
    "            Predicted multi-class targets.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'estimators_')\n",
    "        if (hasattr(self.estimators_[0], \"decision_function\") and\n",
    "                is_classifier(self.estimators_[0])):\n",
    "            thresh = 0\n",
    "        else:\n",
    "            thresh = .5\n",
    "\n",
    "        n_samples = _num_samples(X)\n",
    "        if self.label_binarizer_.y_type_ == \"multiclass\":\n",
    "            maxima = np.empty(n_samples, dtype=float)\n",
    "            maxima.fill(-np.inf)\n",
    "            argmaxima = np.zeros(n_samples, dtype=int)\n",
    "            for i, e in enumerate(self.estimators_):\n",
    "                pred = _predict_binary(e, X)\n",
    "                np.maximum(maxima, pred, out=maxima)\n",
    "                argmaxima[maxima == pred] = i\n",
    "            return self.classes_[np.array(argmaxima.T)]\n",
    "        else:\n",
    "            indices = array.array('i')\n",
    "            indptr = array.array('i', [0])\n",
    "            for e in self.estimators_:\n",
    "                indices.extend(np.where(_predict_binary(e, X) > thresh)[0])\n",
    "                indptr.append(len(indices))\n",
    "            data = np.ones(len(indices), dtype=int)\n",
    "            indicator = sp.csc_matrix((data, indices, indptr),\n",
    "                                      shape=(n_samples, len(self.estimators_)))\n",
    "            return self.label_binarizer_.inverse_transform(indicator)\n",
    "\n",
    "    @if_delegate_has_method(['_first_estimator', 'estimator'])\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Probability estimates.\n",
    "        The returned estimates for all classes are ordered by label of classes.\n",
    "        Note that in the multilabel case, each sample can have any number of\n",
    "        labels. This returns the marginal probability that the given sample has\n",
    "        the label in question. For example, it is entirely consistent that two\n",
    "        labels both have a 90% probability of applying to a given sample.\n",
    "        In the single label multiclass case, the rows of the returned matrix\n",
    "        sum to 1.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "        Returns\n",
    "        -------\n",
    "        T : (sparse) array-like, shape = [n_samples, n_classes]\n",
    "            Returns the probability of the sample for each class in the model,\n",
    "            where classes are ordered as they are in `self.classes_`.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'estimators_')\n",
    "        # Y[i, j] gives the probability that sample i has the label j.\n",
    "        # In the multi-label case, these are not disjoint.\n",
    "        Y = np.array([e.predict_proba(X)[:, 1] for e in self.estimators_]).T\n",
    "\n",
    "        if len(self.estimators_) == 1:\n",
    "            # Only one estimator, but we still want to return probabilities\n",
    "            # for two classes.\n",
    "            Y = np.concatenate(((1 - Y), Y), axis=1)\n",
    "\n",
    "        if not self.multilabel_:\n",
    "            # Then, probabilities should be normalized to 1.\n",
    "            Y /= np.sum(Y, axis=1)[:, np.newaxis]\n",
    "        return Y\n",
    "\n",
    "    @if_delegate_has_method(['_first_estimator', 'estimator'])\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Returns the distance of each sample from the decision boundary for\n",
    "        each class. This can only be used with estimators which implement the\n",
    "        decision_function method.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "        Returns\n",
    "        -------\n",
    "        T : array-like, shape = [n_samples, n_classes]\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'estimators_')\n",
    "        if len(self.estimators_) == 1:\n",
    "            return self.estimators_[0].decision_function(X)\n",
    "        return np.array([est.decision_function(X).ravel()\n",
    "                         for est in self.estimators_]).T\n",
    "\n",
    "    @property\n",
    "    def multilabel_(self):\n",
    "        \"\"\"Whether this is a multilabel classifier\"\"\"\n",
    "        return self.label_binarizer_.y_type_.startswith('multilabel')\n",
    "\n",
    "    @property\n",
    "    def n_classes_(self):\n",
    "        return len(self.classes_)\n",
    "\n",
    "    @property\n",
    "    def coef_(self):\n",
    "        check_is_fitted(self, 'estimators_')\n",
    "        if not hasattr(self.estimators_[0], \"coef_\"):\n",
    "            raise AttributeError(\n",
    "                \"Base estimator doesn't have a coef_ attribute.\")\n",
    "        coefs = [e.coef_ for e in self.estimators_]\n",
    "        if sp.issparse(coefs[0]):\n",
    "            return sp.vstack(coefs)\n",
    "        return np.vstack(coefs)\n",
    "\n",
    "    @property\n",
    "    def intercept_(self):\n",
    "        check_is_fitted(self, 'estimators_')\n",
    "        if not hasattr(self.estimators_[0], \"intercept_\"):\n",
    "            raise AttributeError(\n",
    "                \"Base estimator doesn't have an intercept_ attribute.\")\n",
    "        return np.array([e.intercept_.ravel() for e in self.estimators_])\n",
    "\n",
    "    @property\n",
    "    def _pairwise(self):\n",
    "        \"\"\"Indicate if wrapped estimator is using a precomputed Gram matrix\"\"\"\n",
    "        return getattr(self.estimator, \"_pairwise\", False)\n",
    "\n",
    "    @property\n",
    "    def _first_estimator(self):\n",
    "        return self.estimators_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter num_spin_reversal_transforms for estimator OneVsRestClassifier(estimator=<qboost.qboost.QBoostClassifier object at 0x113fdefd0>,\n          n_jobs=1). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-876615c38a6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneVsRestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQBoostClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_WEAK_CLASSIFIERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTREE_DEPTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mDW_PARAMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Home/anaconda3/envs/env_py2.7/lib/python2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    272\u001b[0m                                  \u001b[0;34m'Check the list of available parameters '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                                  \u001b[0;34m'with `estimator.get_params().keys()`.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m                                  (key, self))\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdelim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter num_spin_reversal_transforms for estimator OneVsRestClassifier(estimator=<qboost.qboost.QBoostClassifier object at 0x113fdefd0>,\n          n_jobs=1). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "NUM_READS = 1000\n",
    "NUM_WEAK_CLASSIFIERS = 30\n",
    "TREE_DEPTH = 4\n",
    "DW_PARAMS = {'num_reads': NUM_READS,\n",
    "            'auto_scale': True,\n",
    "            'num_spin_reversal_transforms': 10,\n",
    "            'postprocess': 'optimization',\n",
    "            'dwave_sampler' : DWaveSampler(token=sapi_token, endpoint = url),\n",
    "            'emb_sampler' : EmbeddingComposite(dwave_sampler)\n",
    "            }\n",
    "set_params(**params)\n",
    "# define sampler\n",
    "dwave_sampler = DWaveSampler(token=sapi_token, endpoint = url)\n",
    "emb_sampler = EmbeddingComposite(dwave_sampler)\n",
    "\n",
    "lmd = 1.0\n",
    "\n",
    "classifier = OneVsRestClassifier(QBoostClassifier(n_estimators=NUM_WEAK_CLASSIFIERS, max_depth=TREE_DEPTH))\n",
    "y_score = classifier.fit(X_train, y_train, emb_sampler, lmd=lmd, **DW_PARAMS)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print y_pred, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
