{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QML Hackathon \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./data/test.csv')\n",
    "test_id = test.id\n",
    "\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "\n",
    "train.rename(columns={\n",
    "    'spacegroup' : 'sg',\n",
    "    'number_of_total_atoms' : 'Natoms',\n",
    "    'percent_atom_al' : 'x_Al',\n",
    "    'percent_atom_ga' : 'x_Ga',\n",
    "    'percent_atom_in' : 'x_In',\n",
    "    'lattice_vector_1_ang' : 'a',\n",
    "    'lattice_vector_2_ang' : 'b',\n",
    "    'lattice_vector_3_ang' : 'c',\n",
    "    'lattice_angle_alpha_degree' : 'alpha',\n",
    "    'lattice_angle_beta_degree' : 'beta',\n",
    "    'lattice_angle_gamma_degree' : 'gamma',\n",
    "    'formation_energy_ev_natom' : 'E',\n",
    "    'bandgap_energy_ev' : 'Eg'}, inplace=True)\n",
    "\n",
    "test.rename(columns={\n",
    "    'spacegroup' : 'sg',\n",
    "    'number_of_total_atoms' : 'Natoms',\n",
    "    'percent_atom_al' : 'x_Al',\n",
    "    'percent_atom_ga' : 'x_Ga',\n",
    "    'percent_atom_in' : 'x_In',\n",
    "    'lattice_vector_1_ang' : 'a',\n",
    "    'lattice_vector_2_ang' : 'b',\n",
    "    'lattice_vector_3_ang' : 'c',\n",
    "    'lattice_angle_alpha_degree' : 'alpha',\n",
    "    'lattice_angle_beta_degree' : 'beta',\n",
    "    'lattice_angle_gamma_degree' : 'gamma',\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'sg', 'Natoms', 'x_Al', 'x_Ga', 'x_In', 'a', 'b', 'c', 'alpha',\n",
       "       'beta', 'gamma', 'E', 'Eg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_all_data = pd.read_csv('./data/all_data.csv')\n",
    "new_all_data.columns\n",
    "all_data = new_all_data\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2400, 45), (2400,), (2400,), (2400, 2), (600, 45))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# features to use\n",
    "features = ['x_Al', 'x_Ga', 'x_In', 'a', 'b', 'c', 'alpha', 'beta',\n",
    "            'gamma', 'vol', 'atomic_density', 'x_Al_avg','x_Ga_avg', 'x_In_avg', 'a_avg',\n",
    "            'b_avg', 'c_avg', 'vol_avg', 'atomic_density_avg', 'pca_abc', 'pca_AlGaInDensity',\n",
    "            'O_0_0','O_1_0', 'O_2_0', 'O_3_0', 'O_4_0', 'O_5_0', 'Al_0_0', 'Al_1_0', 'Al_2_0', 'Al_3_0', 'Al_4_0', 'Al_5_0', 'Ga_0_0',\n",
    "            'Ga_1_0', 'Ga_2_0', 'Ga_3_0', 'Ga_4_0', 'Ga_5_0', 'In_0_0', 'In_1_0',\n",
    "            'In_2_0', 'In_3_0', 'In_4_0', 'In_5_0',]\n",
    "\n",
    "# two different vectors for pca\n",
    "vector1 = all_data[['a', 'b', 'c']].values\n",
    "vector2 = all_data[['x_Al', 'x_Ga', 'x_In', 'atomic_density_avg']].values\n",
    "\n",
    "# use pca to add new features\n",
    "pca = PCA()\n",
    "pca.fit(vector1)\n",
    "all_data['pca_abc'] = pca.transform(vector1)[:,0]\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(vector2)\n",
    "all_data['pca_AlGaInDensity'] = pca.transform(vector2)[:,0]\n",
    "\n",
    "# scaling the data. Linear models tend to like more normally distributed\n",
    "# I tried training on non-scaled, with slightly worse results\n",
    "scale = StandardScaler()\n",
    "scaled = scale.fit(all_data[features]).transform(all_data[features])\n",
    "\n",
    "X_scale = scaled[:train.shape[0]]\n",
    "X_scaled_test = scaled[train.shape[0]:]\n",
    "\n",
    "X_tr = all_data[:train.shape[0]][features].values\n",
    "X_te = all_data[train.shape[0]:][features].values\n",
    "\n",
    "y1 = np.log1p(train['E'])\n",
    "y2 = np.log1p(train['Eg'])\n",
    "\n",
    "y12 = np.column_stack((y1, y2))\n",
    "\n",
    "X_tr.shape, y1.shape, y2.shape, y12.shape, X_scaled_test.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preformance Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# performance matric\n",
    "def rmsle(h, y): \n",
    "    \"\"\"\n",
    "    Compute the Root Mean Squared Log Error for hypthesis h and targets y\n",
    "\n",
    "    Args:\n",
    "        h - numpy array containing predictions with shape (n_samples, n_targets)\n",
    "        y - numpy array containing targets with shape (n_samples, n_targets)\n",
    "    \"\"\"\n",
    "    \n",
    "#     h, y = np.expm1(h), np.expm1(y)\n",
    "    \n",
    "    return np.sqrt(np.square(np.log(h + 1) - np.log(y + 1)).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run different model for different Target Variables\n",
    "\n",
    "grad_1 = GradientBoostingRegressor(\n",
    "                loss='ls',\n",
    "                learning_rate = 0.0035,\n",
    "                max_depth=7,\n",
    "                n_estimators=1120,\n",
    "                max_features=7,\n",
    "                min_samples_leaf=43,\n",
    "                min_samples_split=14,\n",
    "                min_weight_fraction_leaf=0.01556)\n",
    "\n",
    "grad_2 = GradientBoostingRegressor(\n",
    "                loss='ls',\n",
    "                learning_rate = 0.0035,\n",
    "                max_depth=6,\n",
    "                n_estimators=3275,\n",
    "                max_features=2,\n",
    "                min_samples_leaf=2,\n",
    "                min_samples_split=2,\n",
    "                min_weight_fraction_leaf=0.08012)\n",
    "\n",
    "def assess_grad(X, y_list, model_list):\n",
    "    \"\"\" Used to access model performance. Returns the mean rmsle score of cross validated data\n",
    "    \"\"\"\n",
    "    final = []\n",
    "    best_iter = [[], []]\n",
    "    for idx, y in enumerate(y_list):\n",
    "        kfold = KFold(n_splits=10, shuffle=True)\n",
    "        out = []\n",
    "        for train_index, test_index in kfold.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            model = model_list[idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            h =  model.predict(X_test)\n",
    "            e = rmsle(np.expm1(h), np.expm1(y_test))\n",
    "            print(e)\n",
    "            out.append(e)\n",
    "        final.append(np.array(out).mean())\n",
    "                      \n",
    "    return(np.array(final).mean(), np.array(final).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.031338877035052316\n",
      "0.036786384678251766\n",
      "0.03270156805462115\n",
      "0.029722491211051603\n",
      "0.03391982611357298\n",
      "0.03165408561261728\n",
      "0.03131806703884956\n",
      "0.030322693191187612\n",
      "0.03709876982022632\n",
      "0.029283740172801186\n",
      "0.09975723255083838\n",
      "0.10605987336021581\n",
      "0.08996447326140848\n",
      "0.1037877995593643\n",
      "0.0856167309543664\n",
      "0.09401071771707785\n",
      "0.08497277693514203\n",
      "0.09129038040806911\n",
      "0.06721909405572525\n",
      "0.08865171918445909\n",
      "Model RMSLE: 0.06177386504574492, std: 0.029359214752921747\n"
     ]
    }
   ],
   "source": [
    "model = assess_grad(X_tr, [y1, y2], [grad_1, grad_2])\n",
    "print(\"Model RMSLE: {}, std: {}\".format(model[0], model[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgb_1 = XGBRegressor(\n",
    "    learning_rate=0.005,\n",
    "    n_jobs=3,\n",
    "    n_estimators= 1804,\n",
    "    gamma= 0.0,\n",
    "    subsample= 0.222159,\n",
    "    colsample_bytree= 0.5359,\n",
    "    colsample_bylevel= 0.19958,\n",
    "    max_delta_step= 64,\n",
    "    max_depth=28,\n",
    "    min_child_weight= 10,\n",
    "    reg_lambda=0.33038,\n",
    "    silent= True,\n",
    ")\n",
    "\n",
    "xgb_2 = XGBRegressor(\n",
    "    learning_rate=0.005,\n",
    "    n_jobs=3,\n",
    "    n_estimators= 2386,\n",
    "    gamma= 0.0,\n",
    "    subsample= 0.90919,\n",
    "    colsample_bytree= 0.59049,\n",
    "    colsample_bylevel= 0.59404,\n",
    "    max_delta_step= 99,\n",
    "    max_depth=58,\n",
    "    min_child_weight= 85,\n",
    "    reg_lambda= 0.031165789070644215,\n",
    "    silent= True,\n",
    ")\n",
    "def assess_xgb(X, y_list, model_num):\n",
    "    \"\"\" Used to access model performance. Returns the mean rmsle score of cross validated data\n",
    "    \"\"\"\n",
    "    final = []\n",
    "    best_iter = [[], []]\n",
    "    for idx, y in enumerate(y_list):\n",
    "        kfold = KFold(n_splits=10, shuffle=True)\n",
    "        out = []\n",
    "        for train_index, test_index in kfold.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            model = model_num[idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            h =  model.predict(X_test)\n",
    "            e = rmsle(np.expm1(h), np.expm1(y_test))\n",
    "            print('RMSLE: {}'.format(e))\n",
    "            out.append(e)\n",
    "        final.append(np.array(out).mean())\n",
    "    return(np.array(final).mean(), np.array(final).std())\n",
    "\n",
    "model = assess_xgb(X_tr, [y1, y2], [xgb_1, xgb_2])\n",
    "print(\"Model RMSLE: {}, std: {}\".format(model[0], model[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
